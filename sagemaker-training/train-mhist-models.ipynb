{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb6fb2c-9c71-40a9-88e8-e36c5f0dded1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ec2-user/SageMaker/histology-image-analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "IMAGES_DIR = os.environ['SM_CHANNEL_TRAINING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1ce05-89ef-40a6-b527-e0fb3a5ba093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Perplexity.ai\n",
    "# When creating your PyTorch estimator,\n",
    "# specify the S3 location of your data as an input channel:\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='m5.xlarge',\n",
    "    # framework_version='1.8.1',\n",
    "    # py_version='py3',\n",
    "    input_mode='File',\n",
    "    inputs={'training': 's3://mhist-streamlit-app/images/original/'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088f7ae",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5d59a5-408f-49f1-9fa9-c3073eba1d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape (2175, 3)\n",
      "test_df.shape (977, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(            name  experts  label\n",
       " 0  MHIST_aaa.png        6      1\n",
       " 1  MHIST_aab.png        0      0\n",
       " 2  MHIST_aac.png        5      1\n",
       " 3  MHIST_aae.png        1      0\n",
       " 4  MHIST_aaf.png        5      1,\n",
       "             name  experts  label\n",
       " 0  MHIST_aag.png        2      0\n",
       " 1  MHIST_aah.png        2      0\n",
       " 2  MHIST_aaq.png        5      1\n",
       " 3  MHIST_aar.png        0      0\n",
       " 4  MHIST_aay.png        1      0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV files\n",
    "import pandas as pd\n",
    "\n",
    "# image codes are 3 letters long\n",
    "# 'name' : MHIST_<code>.png\n",
    "# 'label' = HP or SSA # binary, categorical label\n",
    "# 'experts' = (int) 0 through 7\n",
    "# 'code' = 3-letter image code\n",
    "\n",
    "# Training set samples: 2175\n",
    "# Test set samples: 977\n",
    "train_df = pd.read_csv('training/trainset_info.csv')\n",
    "test_df = pd.read_csv('training/testset_info.csv')\n",
    "print('train_df.shape', train_df.shape)\n",
    "print('test_df.shape', test_df.shape)\n",
    "train_df.head(), test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c755412e-4fc5-4535-aac6-05e90eaec409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Don't resize nor crop. These are medical images, so we don't want to lose\n",
    "# image integrity. Also, most models, like ViT expect images to be 224x224 pixels.\n",
    "\n",
    "# ToTensor: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a\n",
    "# torch.FloatTensor with shape (C x H x W) in the range [0.0, 1.0]\n",
    "\n",
    "# For Normalize: (calculated from the training data per channel)\n",
    "train_mean = [0.738, 0.649, 0.775]\n",
    "train_std =  [0.197, 0.244, 0.17]\n",
    "\n",
    "# Flatten data for FC\n",
    "DEFAULT_FC_TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "\n",
    "# Don't need to flatten our 2-D, 3-channel image data for ViT\n",
    "DEFAULT_VIT_TRANSFORMS = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "])\n",
    "\n",
    "# We don't need to use a different transform for test sets here because\n",
    "# we're only pre-processing images, not adding synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6859ff-90a5-4f4e-8232-9b3c0ea9e68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset class:\n",
    "class MHIST_dataset(Dataset):\n",
    "    def __init__(self, df, images_dir=IMAGES_DIR, transform=None):\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        if transform == None:\n",
    "            print(\"Error: missing transform for MHIST_dataset\")\n",
    "            raise ValueError(\"Error: missing transform for MHIST_dataset\")\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    # getitem() returns {image tensor, label int64, image filename}\n",
    "    def __getitem__(self, idx):\n",
    "        # df['label'] = 0 or 1 (int64)\n",
    "        # df['name'] ex: MHIST_abc.png\n",
    "        row = self.df.iloc[idx]\n",
    "        full_path = os.path.join(self.images_dir, row['name'])\n",
    "        image_PIL = Image.open(full_path).convert('RGB')\n",
    "        if image_PIL is None:\n",
    "            raise FileNotFoundError(full_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image_PIL) # includes ToTensor\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': row['label'],\n",
    "            'filename': row['name'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4edd6",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eef40b",
   "metadata": {},
   "source": [
    "### Simple FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225e210-bf6a-4a28-a325-46a70af7fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "# Number of features: 224*224*3= 150528\n",
    "\n",
    "class SimpleFC(Module):\n",
    "    def __init__(self,D_in,H1,H2,H3,D_out):\n",
    "        super().__init__()\n",
    "        self.layer1 = Linear(D_in,H1)\n",
    "        self.layer2 = Linear(H1,H2)\n",
    "        self.layer3 = Linear(H2,H3)\n",
    "        self.outlayer = Linear(H3,D_out)\n",
    "    def forward(self,x):\n",
    "        x = relu(self.layer1(x))\n",
    "        x = relu(self.layer2(x))\n",
    "        x = relu(self.layer3(x))\n",
    "        return self.outlayer(x)\n",
    "\n",
    "fc = SimpleFC(150528, 2352, 2352, 294, 1)\n",
    "fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34536575",
   "metadata": {},
   "source": [
    "### **ViT**\n",
    "I used TIMM to download the ViT model and change the output to binary. I set `pretrained = True` and froze all except the head to fine-tune the model with pretrained weights. This version is ViT base and it breaks (224 x 224) images into 16 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd56de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm # CPU-only version is: timm[torch-cpu]\n",
    "import timm\n",
    "VIT_MODEL_TYPE = 'vit_base_patch16_224.augreg2_in21k_ft_in1k'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9554ff9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ed4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MLflow tracking server\n",
    "\n",
    "!pip install -q mlflow\n",
    "!pip install -q boto3\n",
    "!pip install -q awscli\n",
    "!aws configure\n",
    "!aws --version # aws-cli/1.33.15 Python/3.10.12 Linux/6.1.85+ botocore/1.34.133\n",
    "import mlflow\n",
    "'mlflow', mlflow.__version__ # 2.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# For data\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# # For training\n",
    "# TRAIN_DF = pd.read_csv('artifacts/old_train_df.csv')\n",
    "# TEST_DF = pd.read_csv('artifacts/old_test_df.csv')\n",
    "\n",
    "# For inference\n",
    "TRAIN_DF = None\n",
    "TEST_DF = pd.read_csv('artifacts/old_test_df.csv')\n",
    "\n",
    "# For training\n",
    "import torch\n",
    "print('torch.version', torch.__version__)\n",
    "print('torch.version.cuda', torch.version.cuda)\n",
    "print('torch.backends.cudnn.version', torch.backends.cudnn.version())\n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Conv2d, BCEWithLogitsLoss, init # for init.xavier_uniform_\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
    "\n",
    "# For tracking\n",
    "# import mlflow # imported above to get version number\n",
    "MLFLOW_SERVER = \"http://13.52.243.246:5000\"\n",
    "MLFLOW_MODEL_PATH = 'onnx_artifacts' #1.4G\n",
    "MLFLOW_DEFAULT_EXPERIMENT = 'MHIST FC (binary classification)'\n",
    "\n",
    "# checkpoints are saved in PyTorch format (by using torch.save model.state_dict)\n",
    "# uses current device (CPU or GPU)\n",
    "# FC model should have a single output of logits (positive_prob = sigmoid(logit))\n",
    "DEFAULT_MODEL_PATH = 'MHIST_model.pt' # Relative path for saving and loading checkpoints\n",
    "DEFAULT_LR = 1e-6 #1e-4 #2.5e4\n",
    "DROP_LAST_BATCH = False # The dataset size might not divisible by the batch size\n",
    "\n",
    "# To save confusion matrix as a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TrainingSession:\n",
    "  def __init__(self,\n",
    "               train_transform=None,\n",
    "               val_transform=None,\n",
    "               path_for_resuming = None, # resume from local path if not None (not used for saving best model)\n",
    "               resume_from_object = False, # resume from model object\n",
    "               model = None, # if not resume_from_object, FC model is initialized to random\n",
    "               model_type = 'FC', # 'FC' or 'VIT'\n",
    "               batch_size = None,\n",
    "               eval_on='loss',\n",
    "               enable_tracking=False,\n",
    "               logger=None,\n",
    "               ):\n",
    "    self.model_type = model_type\n",
    "    self.logger = logging.getLogger(DEFAULT_LOGGER_NAME) if logger is None else logger\n",
    "    self.enable_tracking = enable_tracking\n",
    "\n",
    "    # Training Setup\n",
    "    self.random = 42\n",
    "    self.label_names = ['HP', 'SSA'] # HP = 0, SSA = 1\n",
    "    self.learning_rate = None # will be set before training\n",
    "    self.optimizer = None     # will be set before training\n",
    "    self.scheduler = None     # will be set before training\n",
    "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # <class 'torch.device'>\n",
    "    self.batch_size = self.__get_batch_size(batch_size)\n",
    "    self.train_transform, self.val_transform = self.__get_transforms(train_transform, val_transform) # use default for None\n",
    "    self.loader_dict = self.__get_loaders(TRAIN_DF, TEST_DF) # might return None or dict['train'] = None\n",
    "    self.model, self.best_model_wts = self.__model_init(path_for_resuming, resume_from_object, model) # resume from path if it is not None\n",
    "\n",
    "    # For saving the best model\n",
    "    self.eval_on = eval_on      # eval model on this metric (can't change this later)\n",
    "    self.best_metric = None     # will be set before training\n",
    "    self.best_model_dest = None # will be set before training\n",
    "    # self.save_total_limit=5,  # 4 most recent models, plus best, when load_best_model_at_end=True\n",
    "    self.precision_goal = 0.8 # 0.8152\n",
    "    self.recall_goal = 0.8 # 0.8039\n",
    "    self.f1_goal = 0.8 # 0.8178\n",
    "    self.accuracy_goal = 0.8 # 0.8192\n",
    "\n",
    "\n",
    "  def __get_batch_size(self, batch_size):\n",
    "      if batch_size is None:\n",
    "        print('Using default batch size')\n",
    "        return 32 if torch.cuda.is_available() else 4 # BATCH_SIZE = 256 # for A100\n",
    "      return batch_size\n",
    "\n",
    "\n",
    "  def __get_transforms(self, train_transform, val_transform): # check for None values\n",
    "    print('Setting up data:')\n",
    "    if self.model_type == 'FC':\n",
    "        print('Using FC transforms')\n",
    "        if train_transform is None:\n",
    "          train_transform = DEFAULT_FC_TRANSFORMS\n",
    "        if val_transform is None:\n",
    "          val_transform = DEFAULT_FC_TRANSFORMS\n",
    "\n",
    "    elif self.model_type == 'VIT':\n",
    "        print('Using ViT transforms')\n",
    "        if train_transform is None:\n",
    "          train_transform = DEFAULT_VIT_TRANSFORMS\n",
    "        if val_transform is None:\n",
    "          val_transform = DEFAULT_VIT_TRANSFORMS\n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "  def __get_loaders(self, train_df=None, test_df=None):\n",
    "    samples_weights = np.load('artifacts/samples_weights.npy')\n",
    "    sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "\n",
    "    if test_df is None:\n",
    "      print(\"Error: can't evaluate or run inference without test_df.\")\n",
    "      return None\n",
    "\n",
    "    elif train_df is None: # don't need to shuffle\n",
    "      val_loader = DataLoader(MHIST_dataset(test_df, transform=self.val_transform),\n",
    "                                batch_size=self.batch_size, shuffle=False, drop_last=DROP_LAST_BATCH)\n",
    "      loader_dict = {'train':None, 'val':val_loader}\n",
    "      test_samples = len(test_df) # loader has length = num_batches\n",
    "      print(f\"No training set. Model is ready for inference on {test_samples} samples.\")\n",
    "      print('batch_size', self.batch_size, 'val_num_batches', math.floor(test_samples/self.batch_size))\n",
    "      return loader_dict # Training loader is empty!\n",
    "\n",
    "    else: # we have train_df and test_df\n",
    "      print('train_df len', len(train_df), 'test_df len', len(test_df))\n",
    "      train_loader = DataLoader(MHIST_dataset(train_df, transform=self.train_transform),\n",
    "                                batch_size=self.batch_size, sampler=sampler, drop_last=DROP_LAST_BATCH)\n",
    "      val_loader = DataLoader(MHIST_dataset(test_df, transform=self.val_transform),\n",
    "                                batch_size=self.batch_size, shuffle=True, drop_last=DROP_LAST_BATCH)\n",
    "      loader_dict = {'train':train_loader, 'val':val_loader}\n",
    "\n",
    "      train_samples = len(train_df)\n",
    "      test_samples = len(test_df)\n",
    "      print(f\"Training with {len(loader_dict['train'])} batches, validating with {len(loader_dict['val'])} batches with batch_size {self.batch_size}\")\n",
    "      print('train_num_batches', math.ceil(train_samples/self.batch_size), 'and val_num_batches', math.ceil(test_samples/self.batch_size))\n",
    "      return loader_dict\n",
    "\n",
    "      # print(\"\\nExample image:\")\n",
    "      # import random\n",
    "      # rand_img_path = IMAGES_DIR+random.choice(annotations['Image Name'])\n",
    "      # print('Random image path:', rand_img_path)\n",
    "      # visualize_scan(rand_img_path)\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def initialize_weights(module):\n",
    "      if type(module) == Linear or type(module) == Conv2d:\n",
    "          init.xavier_uniform_(module.weight)\n",
    "\n",
    "\n",
    "  # Log gradient summary before training\n",
    "  def __grad_summary(self):\n",
    "      if mhist_logger.getEffectiveLevel() <= logging.WARNING:\n",
    "          layers = list(self.model.children())\n",
    "          total_layers_training = 0\n",
    "          # print('Model layer gradients:\\n', layers)\n",
    "\n",
    "          # Get the parameters of the first three layers\n",
    "          for i, layer in enumerate(layers, 1):\n",
    "              # count total tensors and tensors that require grad\n",
    "              print(f\"Layer {i}:\")\n",
    "              total_tensors = 0\n",
    "              num_training = 0\n",
    "              for param in layer.parameters():\n",
    "                  total_tensors += 1 # two tensors per linear layer: weights and biases\n",
    "                  if param.requires_grad:\n",
    "                      num_training += 1\n",
    "                  # print(param.dtype) # Debug\n",
    "                  # print(param.shape) # Debug\n",
    "\n",
    "              if num_training==0 and total_tensors>0: # fully frozen\n",
    "                  print(f\"  frozen\")\n",
    "              elif total_tensors>0:\n",
    "                  total_layers_training += 1\n",
    "                  print(f\"  tensors that require grad: {num_training}/{total_tensors}\") # Info\n",
    "              else:\n",
    "                  print(f\"  no tensors\")\n",
    "          print('---------------------------------')\n",
    "          print('Training', total_layers_training, 'layers of', len(layers), 'total')\n",
    "\n",
    "\n",
    "  def __freeze_all_but_head(self):\n",
    "      # Freeze all params\n",
    "      for param in self.model.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "      # Unfreeze last layer\n",
    "      if hasattr(self.model, 'head'):\n",
    "          for param in self.model.head.parameters():\n",
    "              param.requires_grad = True\n",
    "      else:\n",
    "          print(\"Error: Can't unfreeze classification head\")\n",
    "\n",
    "      # # Check grads\n",
    "      # for name, param in model.named_parameters():\n",
    "      #     print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "\n",
    "      # For large models\n",
    "      # train_dense_w_and_b = sum([1 if param.requires_grad else 0 for param in self.model.classifier.dense.parameters()])\n",
    "      # train_out_w_and_b = sum([1 if param.requires_grad else 0 for param in self.model.classifier.out_proj.parameters()])\n",
    "\n",
    "      # if train_dense_w_and_b == 0 and train_out_w_and_b == 0:\n",
    "      #     logger.debug('\\nClassification head is frozen.')\n",
    "      # elif train_dense_w_and_b == 2 and train_out_w_and_b == 2:\n",
    "      #     logger.debug('\\nFine-tuning the classification head.')\n",
    "\n",
    "      # else: # Some params in classification head are being trained and some aren't\n",
    "      #     if train_dense_w_and_b == 0:\n",
    "      #         dense = 'Final dense layer is frozen'\n",
    "      #     elif train_dense_w_and_b == 1:\n",
    "      #         dense = 'Warning: fine-tuning some, but not all params in dense layer'\n",
    "      #     else: # train_dense_w_and_b == 2:\n",
    "      #         dense = 'Fine-tuning final dense layer'\n",
    "\n",
    "      #     if train_out_w_and_b == 0:\n",
    "      #         out = ', but output projection layer is frozen.'\n",
    "      #     elif train_out_w_and_b == 1:\n",
    "      #         out = 'and Warning: fine-tuning some, but not all params in output projection layer.'\n",
    "      #     else: # train_out_w_and_b == 2:\n",
    "      #         out = ', but fine-tuning output projection layer.'\n",
    "      #     logger.debug(dense + out)\n",
    "\n",
    "\n",
    "  def __alloc_new_model(self, model_obj): # this won't init weights\n",
    "      if model_obj is None and self.model_type == 'FC':\n",
    "        print('Allocating a new SimpleFC')\n",
    "        model_obj = SimpleFC(150528, 2352, 2352, 294, 1)\n",
    "      elif model_obj is None and self.model_type == 'VIT':\n",
    "        model_obj = timm.create_model(\n",
    "            model_name=VIT_MODEL_TYPE,\n",
    "            pretrained=True,\n",
    "            num_classes=1, # change number of outputs in classification head\n",
    "        )\n",
    "        print(f\"Allocating a new model from timm: {model_obj.default_cfg['architecture']} with pretrained weight tag: {model_obj.default_cfg['tag']}\")\n",
    "      return model_obj\n",
    "\n",
    "  def __model_init(self, path_for_resuming, resume_from_object, model_obj): # resume from path if it is not None\n",
    "    # If we're resuming from object and path, resuming from object is given priority\n",
    "    # This way, we won't overwrite memory, if we accidentally pass in a path as well\n",
    "    # If we meant to resume from a path, we can always retry because it's on disk\n",
    "    if resume_from_object: # else use model passed into the training method without altering the weights\n",
    "      if model_obj is None:\n",
    "          print(\"Warning: Can't resume because model = None\")\n",
    "          return None, None\n",
    "      print('Info: Resuming from model object')\n",
    "      # model_obj returned below\n",
    "\n",
    "    elif path_for_resuming is not None: # Load weights from disk\n",
    "      model_obj = self.__alloc_new_model(model_obj) # based on self.model_type\n",
    "      print('Info: Resuming from saved state dict', path_for_resuming)\n",
    "      model_obj.load_state_dict(torch.load(path_for_resuming)) # model weights might not work with default model, above\n",
    "\n",
    "    # Create default model (not resuming from object nor path)\n",
    "    elif model_obj is None:\n",
    "      model_obj = self.__alloc_new_model(model_obj) # based on self.model_type\n",
    "      if self.model_type == 'FC': # don't init ViT pretrained\n",
    "          print('Info: Initializing model weights with Xavier Uniform')\n",
    "          model_obj.apply(self.initialize_weights) # Xavier Uniform\n",
    "\n",
    "    # Load into memory, in case this session makes the weights worse\n",
    "    # If not resuming from object, use the model that was passed in (which could be different from the currently loaded model)\n",
    "    print('Loading a copy of initial model weights into memory')\n",
    "    best_model_wts = copy.deepcopy(model_obj.state_dict())\n",
    "\n",
    "    return model_obj, best_model_wts # model to train and copy of initial model weights (random or from checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "                    ### TRAIN/EVAL LOOP ###\n",
    "\n",
    "  # Wrapper for setting params before running train/eval loop\n",
    "  def train_and_evaluate(self, model = None, # (optional) pass a model to freeze/unfreeze, check, or change anything between runs\n",
    "                         epochs = 10,\n",
    "                         freeze_all_but_head = True,\n",
    "                         learning_rate = None,\n",
    "                         best_metric = None,\n",
    "                         best_model_dest = DEFAULT_MODEL_PATH,\n",
    "                         mlflow_experiment = MLFLOW_DEFAULT_EXPERIMENT,\n",
    "                         mlflow_run = None):\n",
    "\n",
    "    if model is not None:\n",
    "        self.model = model\n",
    "    if freeze_all_but_head:\n",
    "        self.__freeze_all_but_head()\n",
    "    self.__grad_summary()\n",
    "\n",
    "    if best_metric is not None:\n",
    "        self.best_metric = best_metric\n",
    "    elif self.best_metric is None and self.eval_on != 'loss':\n",
    "        self.best_metric = 0.\n",
    "    # else: if eval_on == 'loss' then best_metric might be None\n",
    "    self.best_model_dest = best_model_dest\n",
    "    print('Evaluating the model on', self.eval_on, 'best_metric =', self.best_metric)\n",
    "\n",
    "    # Get a list of trainable params to pass into optimizer (for efficiency)\n",
    "    trainable_tensors = [p for p in self.model.parameters() if p.requires_grad]\n",
    "    # Init self with default LR and create a new optimizer\n",
    "    if learning_rate is None and self.learning_rate is None:\n",
    "        self.learning_rate = DEFAULT_LR\n",
    "        self.optimizer = Adam(params=trainable_tensors, lr=DEFAULT_LR)\n",
    "\n",
    "    # Update with new LR (passed into method)\n",
    "    elif learning_rate is not None:\n",
    "        if self.learning_rate is None: # init with LR param and new optimizer\n",
    "            self.optimizer = Adam(params=trainable_tensors, lr=learning_rate)\n",
    "        else: # update existing LR with new one in existing optimizer\n",
    "            self.optimizer.param_groups[0]['lr'] = learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "    # else: param is None, so don't update existing LR and optimizer (which are not None)\n",
    "    print('Initial learning rate', self.learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    if self.enable_tracking:\n",
    "        # __track_training will call __train_eval_loop()\n",
    "        self.__track_training(epochs, mlflow_experiment, mlflow_run)\n",
    "    else: # Don't use MLflow tracking server\n",
    "        print('Info: Starting training without tracking to MLflow server.')\n",
    "        self.__train_eval_loop(epochs)\n",
    "\n",
    "    return self.model # in case we want to freeze/unfreeze, check, or change anything\n",
    "\n",
    "\n",
    "  def __get_model_hyperparams(self):\n",
    "    # ViT input: torch.FloatTensor with shape (C x H x W) in the range [0.0, 1.0]\n",
    "    input_type = {\n",
    "        'FC':\"flattened 3 x 224 x 224 (RGB mode) in range [0.0, 1.0]\",\n",
    "        'VIT':\"3 x 224 x 224 (RGB mode) in range [0.0, 1.0]\",\n",
    "    }\n",
    "    in_feature_shape = 150528 if self.model_type == 'FC' else [3, 224, 224] # (C x H x W)\n",
    "    return dict(\n",
    "      model_class=str(self.model.__class__.__name__),\n",
    "      model_info=str(self.model),\n",
    "      input_image=input_type[self.model_type],\n",
    "      num_in_features=in_feature_shape,\n",
    "      in_feature_dtype = 'torch.float32',\n",
    "      num_outputs=1,\n",
    "      label_names=self.label_names,\n",
    "      test_size='is set with dataframes',\n",
    "      batch_size=self.batch_size,\n",
    "      random=self.random,\n",
    "      device=str(self.device),\n",
    "      eval_on = self.eval_on,\n",
    "      best_metric = self.best_metric,\n",
    "      best_model_dest = self.best_model_dest,\n",
    "      # logger=get_log_filename(self.logger),\n",
    "    )\n",
    "\n",
    "\n",
    "  def __track_training(self, epochs, experiment, run_id):\n",
    "    mlflow.set_tracking_uri(MLFLOW_SERVER)\n",
    "    mlflow.set_experiment(experiment)\n",
    "\n",
    "    # \"with\" manages context by calling mlflow.end_run() appropriately.\n",
    "    # Beforehand, it ends any active run. Upon error or exit, it ends this run.\n",
    "    # run_id = None will start a new run\n",
    "    with mlflow.start_run(run_id) as run:\n",
    "        print('\\nLogging metrics with server:', mlflow.get_tracking_uri())\n",
    "        print('MLflow: run_name =', run.info.run_name, 'run_id =', run.info.run_id) # run_id is a UUID\n",
    "        self.run_id = run.info.run_id # save in case we want to log the model later with self.log_model\n",
    "        # print('Artifacts stored at:', mlflow.get_artifact_uri())\n",
    "        print('MLflow Experiment name:', experiment, '\\n')\n",
    "        mlflow.log_params(self.__get_model_hyperparams())\n",
    "\n",
    "        self.__train_eval_loop(epochs)\n",
    "\n",
    "\n",
    "  def __train_eval_loop(self, epochs):\n",
    "    if self.scheduler is None:\n",
    "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1, last_epoch=-1) # multiply LR by 10% every 5 epochs\n",
    "\n",
    "    self.model.to(self.device)\n",
    "    print('Device:', next(self.model.parameters()).device, 'torch.cuda.device_count:', torch.cuda.device_count())\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    steps = 0\n",
    "    previous_lr = None # To track changes in LR\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Each epoch has a two phases: training and validation\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                self.model.train()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "\n",
    "            # for each epoch phase:\n",
    "            epoch_loss    = 0.\n",
    "            epoch_correct = 0\n",
    "            epoch_samples = 0\n",
    "            y = []      # correct labels\n",
    "            y_pred = [] # pred labels\n",
    "\n",
    "            for batch_idx, batch in enumerate(tqdm(self.loader_dict[phase])):#, total=len(self.loader_dict[phase])):\n",
    "                # batch is a dict of:\n",
    "                # {'image' : tensor of batch_size flattened or [batch,3,224,224] images,\n",
    "                #  'label' : tensor batch_size integer labels}\n",
    "                images = batch['image'].to(self.device) # logits dtype torch.float32\n",
    "                labels = batch['label'].type(torch.FloatTensor).to(self.device) # labels dtype torch.float32\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                # track history only when phase == 'train'\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    batch_size = len(labels)\n",
    "                    logits = self.model(images).squeeze() # output is torch.Size([32, 1])\n",
    "                    loss = criterion(logits, labels) # BCEWithLogitsLoss\n",
    "                    preds = (logits > 0).float() # np.where(logits.squeeze() > 0, 1, 0)\n",
    "\n",
    "                    # DEBUG: shapes and dtypes\n",
    "                    # print('\\nlabels:', labels.shape, 'dtype', labels.dtype) # torch.float32\n",
    "                    # print('logits:', logits.shape, 'dtype', logits.dtype) # torch.float32\n",
    "                    # print('preds:', preds.shape, 'dtype', preds.dtype) # torch.float32\n",
    "                    num_correct = (preds == labels).sum().item() # this will broadcast if shapes don't match\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                        steps += 1\n",
    "\n",
    "                # Update totals and lists\n",
    "                epoch_loss    += loss.item() * batch_size # TODO: check this\n",
    "                epoch_correct += num_correct\n",
    "                epoch_samples += batch_size  # last dataloader batch might be smaller\n",
    "                y.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "                # # DEBUG: First batch (in 'train' phase)\n",
    "                # if epoch == 0 and batch_idx == 0 and phase == 'train':\n",
    "                #   labels_np= labels.cpu().numpy()\n",
    "                #   preds_np= preds.cpu().numpy()\n",
    "                #   logits_np = logits.squeeze().cpu().detach().numpy()\n",
    "                #   f1 = f1_score(y, y_pred, average='weighted') # accepts numpy or list\n",
    "                #   pr_auc = average_precision_score(y, y_pred, average='weighted')\n",
    "                #   recall = recall_score(y, y_pred, pos_label=1) # recall for positive class\n",
    "                #   print('Label, Pred, Logit, Correct')\n",
    "                #   print(np.column_stack((labels_np, preds_np, logits_np, preds_np == labels_np)))\n",
    "                #   print(f'[Epoch {epoch}, Batch {batch_idx}] Batch loss: {loss.item()/batch_size :.3f}, Accuracy: {float(num_correct)/batch_size :.3f} correct, F1 Score: {f1 :.3f}, Recall for 1: {recall:.3f}')\n",
    "                #   print(classification_report(y, y_pred))\n",
    "\n",
    "            # After each epoch phase, update stats:\n",
    "            epoch_metrics = self.__compute_metrics(phase, epoch_loss, epoch_correct, epoch_samples, y, y_pred)\n",
    "            if phase == 'train':\n",
    "              eval_on = self.eval_on\n",
    "              loss_name = 'loss'\n",
    "              f1_name = 'f1'\n",
    "              accuracy_name = 'accuracy'\n",
    "            else:\n",
    "              eval_on = 'val_'+self.eval_on\n",
    "              loss_name = 'val_loss'\n",
    "              f1_name = 'val_f1'\n",
    "              accuracy_name = 'val_accuracy'\n",
    "            if self.enable_tracking:\n",
    "                mlflow.log_metrics(metrics=epoch_metrics, step=steps)\n",
    "            print(f\"Epoch {epoch+1} ({steps} steps) {phase} Loss: {epoch_metrics[loss_name]:.3f}, {eval_on}: {epoch_metrics[eval_on]}, F1: {epoch_metrics[f1_name]} Accuracy: {epoch_metrics[accuracy_name]:.3f}\")\n",
    "\n",
    "            if phase == 'train':\n",
    "                # Step LR scheduler\n",
    "                self.scheduler.step()\n",
    "\n",
    "                # Check if the learning rate has changed\n",
    "                current_lr = self.scheduler.get_last_lr()[0]\n",
    "                if previous_lr is None:\n",
    "                    previous_lr = current_lr\n",
    "                elif current_lr != previous_lr:\n",
    "                    print(f\"Epoch {epoch+1} {phase}: Learning rate changed to {current_lr:.6f}\")\n",
    "                    previous_lr = current_lr\n",
    "\n",
    "        # End of an epoch (train and val)\n",
    "        if epoch==0:\n",
    "            epoch_time = time.time() - start\n",
    "            print(f'Epoch 0 completed in {epoch_time // 60:.0f}m {epoch_time % 60:.0f}s')\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best {self.eval_on} score: {self.best_metric}')\n",
    "    initial_lr = self.optimizer.param_groups[0]['initial_lr']\n",
    "    final_lr = self.scheduler.get_last_lr()[0]\n",
    "    print(f'Learning rate: start={initial_lr} final={final_lr}')\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(cm)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    if self.enable_tracking:\n",
    "        mlflow.log_metric(\"time_elapsed\", time_elapsed, step=steps)\n",
    "        self.__log_dict(cm, step=steps)\n",
    "        self.__save_heatmap(cm) # save image to local and mlflow artifacts\n",
    "        self.__log_hyperparams(self.optimizer, self.scheduler, criterion)\n",
    "\n",
    "    # Load best model weights, in case this run was worse\n",
    "    self.model.load_state_dict(self.best_model_wts)\n",
    "\n",
    "\n",
    "  def __log_hyperparams(self, optimizer, scheduler, criterion):\n",
    "        # Adam optimizer\n",
    "        mlflow.log_params(dict(\n",
    "            optimizer = optimizer.__class__.__name__,\n",
    "            initial_lr = self.optimizer.param_groups[0]['initial_lr'],\n",
    "            final_lr = self.scheduler.get_last_lr()[0],\n",
    "            weight_decay = optimizer.param_groups[0]['weight_decay']\n",
    "            ))\n",
    "\n",
    "        # StepLR Scheduler\n",
    "        mlflow.log_params(dict( # StepLR\n",
    "            scheduler = scheduler.__class__.__name__,\n",
    "            step_size = scheduler.step_size,\n",
    "            gamma = scheduler.gamma,\n",
    "            last_epoch = scheduler.last_epoch,\n",
    "            verbose = scheduler.verbose,\n",
    "        ))\n",
    "\n",
    "        # BCEWithLogitsLoss\n",
    "        mlflow.log_param('loss_function', type(criterion).__name__)\n",
    "\n",
    "\n",
    "  def __log_dict(self, cm, step):\n",
    "      # Rows represent the actual classes\n",
    "      # Columns represent the predicted classes\n",
    "      mlflow.log_metric(\"true_negative\", cm[0][0], step=step)\n",
    "      mlflow.log_metric(\"false_positive\", cm[0][1], step=step)\n",
    "      mlflow.log_metric(\"false_negative\", cm[1][0], step=step)\n",
    "      mlflow.log_metric(\"true_positive\", cm[1][1], step=step)\n",
    "\n",
    "\n",
    "  def __save_heatmap(self, cm_numpy):\n",
    "      plt.figure(figsize=(4, 3))\n",
    "      sns.heatmap(cm_numpy, annot=True, fmt='d', cmap='Blues')\n",
    "      plt.title('Confusion Matrix')\n",
    "      plt.ylabel('True label')\n",
    "      plt.xlabel('Predicted label')\n",
    "\n",
    "      # Save and upload image\n",
    "      model_name = os.path.splitext(os.path.basename(self.best_model_dest))[0] # get filename, remove extension\n",
    "      image_path = f'artifacts/{model_name}_confusion_matrix.png'\n",
    "      plt.savefig(image_path) # it's possible that there won't be a best model checkpoint to match this image\n",
    "      mlflow.log_artifact(image_path)\n",
    "\n",
    "\n",
    "  def __compute_metrics(self, phase, epoch_loss, epoch_correct, epoch_samples, y, y_pred):\n",
    "      # print(f'Computing metrics for {phase}')\n",
    "      # # debug:\n",
    "      # compare_labels = np.column_stack((logits, preds, labels, preds == labels))\n",
    "      tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "      metrics = dict(\n",
    "          loss = epoch_loss/epoch_samples, # average loss per sample (not per batch)\n",
    "          weighted_precision = precision_score(y, y_pred, average='weighted'),\n",
    "          weighted_recall = recall_score(y, y_pred, average='weighted'),\n",
    "          f1 = f1_score(y, y_pred, average='weighted'),\n",
    "          accuracy = accuracy_score(y, y_pred),\n",
    "          false_negative_rate = fn / (fn + tp),\n",
    "          true_positive_rate = tp / (tp + fn) # recall for positive class\n",
    "          # recall = recall_score(y, y_pred, pos_label=1), # recall for positive class\n",
    "      )\n",
    "\n",
    "      if phase == 'val': # check model for improvement (epoch end)\n",
    "          improved = False\n",
    "          # loss improves when minimized, others when maximized\n",
    "          if self.eval_on == 'loss' and self.best_metric is None:\n",
    "              self.best_metric = metrics[self.eval_on]\n",
    "          elif self.eval_on == 'loss' and metrics[self.eval_on] < self.best_metric:\n",
    "              print(f\"Improved validation loss: {metrics['loss']}\")\n",
    "              improved = True\n",
    "          elif self.eval_on != 'loss' and metrics[self.eval_on] > self.best_metric:\n",
    "              print(f'Improved validation {self.eval_on}: {metrics[self.eval_on]}')\n",
    "              improved = True\n",
    "\n",
    "          if improved and self.__met_goal(metrics):\n",
    "              print(f'Saving model weights to {self.best_model_dest}')\n",
    "              torch.save(self.model.state_dict(), self.best_model_dest) #to disk\n",
    "              best_model_wts = copy.deepcopy(self.model.state_dict()) #to memory\n",
    "              self.best_metric = metrics[self.eval_on]\n",
    "\n",
    "          return {'val_'+key: value for key, value in metrics.items()} # add prefix to val metric names\n",
    "      return metrics\n",
    "\n",
    "\n",
    "  def __met_goal(self, metrics_dict):\n",
    "      met_new_goal = False\n",
    "      if metrics_dict['weighted_precision'] > self.precision_goal:\n",
    "          print(f\"Improved validation weighted_precision: {metrics_dict['weighted_precision']}\")\n",
    "          self.precision_goal = metrics_dict['weighted_precision']\n",
    "          met_new_goal = True\n",
    "      if metrics_dict['weighted_recall'] > self.recall_goal:\n",
    "          print(f\"Improved validation weighted_recall: {metrics_dict['weighted_recall']}\")\n",
    "          self.recall_goal = metrics_dict['weighted_recall']\n",
    "          met_new_goal = True\n",
    "      if metrics_dict['f1'] > self.f1_goal:\n",
    "          print(f\"Improved validation F1-score: {metrics_dict['f1']}\")\n",
    "          self.f1_goal = metrics_dict['f1']\n",
    "          met_new_goal = True\n",
    "      if metrics_dict['accuracy'] > self.accuracy_goal:\n",
    "          print(f\"Improved validation accuracy: {metrics_dict['accuracy']}\")\n",
    "          self.accuracy_goal = metrics_dict['accuracy']\n",
    "          met_new_goal = True\n",
    "      return met_new_goal\n",
    "\n",
    "\n",
    "  # Log from path or log self.model\n",
    "  # Pass in run_id or use self.run_id\n",
    "\n",
    "  # Implement logging to mlflow based on best_metric here\n",
    "  # Compate best_metric from runs in mlflow experiment,\n",
    "  # in case there may be a better model already stored on remote.\n",
    "  # If this is a better model than the best model in the experiment, log the artifact.\n",
    "  # user needs to manage the number of models in s3.\n",
    "  # MLflow can store metrics regardless of whether the artifacts are present.\n",
    "  # Serialize as ONNX\n",
    "  # This won't save the model in the MLflow registry.\n",
    "  def log_model(self, path=None, run_id=None):\n",
    "    # self.model\n",
    "    pass\n",
    "\n",
    "\n",
    "  # Download onnx model from MLflow server to local PyTorch checkpoint\n",
    "  # for resuming training or running inference\n",
    "  @staticmethod\n",
    "  def download_model(self, run_id, local_dest=DEFAULT_MODEL_PATH):\n",
    "    # if self.debug in ['debug', 'info']:\n",
    "    #     print(f'Downloading Huggingface model and tokenizer: {pretrained_name}')\n",
    "\n",
    "    # # Huggingface Model:\n",
    "    # # If config.num_labels == 1 a regression loss is computed (Mean-Square loss),\n",
    "    # # If config.num_labels > 1 a classification loss is computed (Cross-Entropy).\n",
    "    # # FB RoBERTa already uses 2 labels, so we don't need to change the model layers (num_labels=2) or ignore the number of weights\n",
    "    # # ignore_mismatched_sizes=True # ignore downloaded model weights for the last layer (out_proj) to output new num_labels\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(pretrained_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "    # model.config.id2label = {i:label for i, label in enumerate(self.labels)}\n",
    "    # model.config.label2id = {label:i for i, label in enumerate(self.labels)}\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(pretrained_name, config=model.config)\n",
    "\n",
    "    # if self.debug in ['debug']:\n",
    "    #     print('Model config problem type:', model.config.problem_type)\n",
    "    #     print(model)\n",
    "\n",
    "    # # # Freeze pretrained model weights/biases\n",
    "    # # for param in model.parameters():\n",
    "    # #     param.requires_grad = False\n",
    "    # #     print('\\nAll layers are frozen')\n",
    "    # # for param in model.classifier.parameters(): # train classification head\n",
    "    # #     param.requires_grad = True\n",
    "    # #     print('except for the classification head.')\n",
    "\n",
    "    # return model\n",
    "    pass\n",
    "\n",
    "  @staticmethod\n",
    "  def __sigmoid(np_outs):\n",
    "      np_outs = np.clip(np_outs, -50, 50) # prevent np.exp overflow for large values (in case of an issue with preprocessing)\n",
    "      return 1 / (1 + np.exp(-np_outs))\n",
    "\n",
    "\n",
    "  # Batch inference: initalize a session with a model, then run this method (not train)\n",
    "  # input df path is hard-coded, above, in global var: TEST_DF\n",
    "  # The following uses numpy, not Pytorch, but it would be faster and more concise to use Pytorch\n",
    "  def batch_predict(self):\n",
    "      self.model.eval()\n",
    "      self.model.to(self.device)\n",
    "      dataset = self.loader_dict['val'].dataset\n",
    "      epoch_np = None\n",
    "      with torch.no_grad():\n",
    "          first_batch = True\n",
    "          for batch in tqdm(self.loader_dict['val']):\n",
    "              images = batch['image'].to(self.device) # torch.Size([960, 3, 224, 224]) torch.float32\n",
    "              # print('inputs.dtype:', images.dtype, 'shape:', images.shape)\n",
    "              logits_np = self.model(images).detach().cpu().squeeze().numpy() # shape=(960,) dtype=float32\n",
    "              # print('logits_np.dtype:', logits_np.dtype, 'shape:', logits_np.shape)\n",
    "\n",
    "              positive_probs_np = self.__sigmoid(logits_np) # shape=(960,) dtype=float32\n",
    "              # print('positive_probs_np.dtype:', positive_probs_np.dtype, 'shape:', positive_probs_np.shape)\n",
    "              preds_np = (positive_probs_np > 0.5).astype(int) # shape=(960,) dtype=int64\n",
    "              # print('preds_np.dtype:', preds_np.dtype, 'shape:', preds_np.shape)\n",
    "\n",
    "              labels_np = batch['label'].cpu().squeeze().numpy() # shape=(960,) dtype=int64\n",
    "              # print('labels_np.dtype:', labels_np.dtype, 'labels_np.shape:', labels_np.shape)\n",
    "              correct_np = np.equal(labels_np, preds_np).astype(int) # shape=(960,) dtype=int64\n",
    "              # print('correct_np.dtype:', correct_np.dtype, 'shape:', correct_np.shape)\n",
    "\n",
    "              # # Store results (keyed on the idx of the image in the dataset)\n",
    "              # idx_np = batch['idx'].cpu().squeeze().numpy()\n",
    "              # print('idx_np.dtype:', idx_np.dtype, 'idx_np.shape:', idx_np.shape)\n",
    "              # for i in range(len(idx_np)):\n",
    "              #     dataset.store_result(\n",
    "              #         idx_np[i].item(),\n",
    "              #         logits_np[i].item(),\n",
    "              #         preds_np[i].item(),\n",
    "              #         positive_probs_np[i].item(),\n",
    "              #         correct_np[i].item()\n",
    "              #     )\n",
    "\n",
    "              filename_np = np.array(batch['filename']) # np objects, shape (960,) filenames are returned as <class 'list'>\n",
    "              if first_batch:\n",
    "                  first_batch = False\n",
    "                  epoch_np = np.column_stack((filename_np, labels_np, logits_np, preds_np, positive_probs_np, correct_np))\n",
    "                  # print(pd.DataFrame(epoch_np, columns=['filename', 'label', 'logit', 'prediction', 'positive_prob', 'correct']))\n",
    "              else:\n",
    "                  batch_np = np.column_stack((filename_np, labels_np, logits_np, preds_np, positive_probs_np, correct_np))\n",
    "                  epoch_np = np.vstack((epoch_np, batch_np))\n",
    "      results_df = pd.DataFrame(epoch_np, columns=['filename', 'label', 'logit', 'prediction', 'positive_prob', 'correct'])\n",
    "\n",
    "      # Convert datatypes to string, float, int\n",
    "      results_df['filename'] = results_df['filename'].astype('string')\n",
    "      results_df['logit'] = pd.to_numeric(results_df['logit'], errors='coerce')\n",
    "      results_df['positive_prob'] = pd.to_numeric(results_df['positive_prob'], errors='coerce')\n",
    "      results_df['label'] = pd.to_numeric(results_df['label'], errors='coerce').astype('Int64')\n",
    "      results_df['prediction'] = pd.to_numeric(results_df['prediction'], errors='coerce').astype('Int64')\n",
    "      results_df['correct'] = pd.to_numeric(results_df['correct'], errors='coerce').astype('Int64')\n",
    "\n",
    "      # calculate metrics\n",
    "      y_true = results_df['label'].to_numpy(dtype=int)\n",
    "      y_prob = results_df['positive_prob'].to_numpy(dtype=int)\n",
    "      y_pred = results_df['prediction'].to_numpy(dtype=int)\n",
    "      tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "      metrics = {\n",
    "          'weighted_precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "          'weighted_recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "          'weighted_f1': f1_score(y_true, y_pred, average='weighted'),\n",
    "          'accuracy': accuracy_score(y_true, y_pred),\n",
    "          'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "          'roc_auc': roc_auc_score(y_true, y_prob),\n",
    "          'pr_auc': average_precision_score(y_true, y_prob),\n",
    "          'false_negative_rate': fn / (fn + tp),\n",
    "          'true_positive_rate': tp / (tp + fn) # recall for positive class\n",
    "      }\n",
    "\n",
    "      return results_df, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce05a6e",
   "metadata": {},
   "source": [
    "### Train FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous best: eval_on f1 , best_metric=0.7669682977041087\n",
    "# Improved validation recall: 0.5482954545454546\n",
    "fc_session = FinetuneFC(\n",
    "              #  train_transform=None,\n",
    "              #  val_transform=None,\n",
    "              #  path_for_resuming = 'artifacts/MHIST_FC_run2', # resume from local path if not None (not used for saving best model)\n",
    "               # resume_from_object = False, # resume from model object\n",
    "               # model = best_model, # if not resume_from_object, model is initialized to random\n",
    "               batch_size = 64 if torch.cuda.is_available() else 4,\n",
    "               eval_on='recall', # for positive (minority) class\n",
    "               enable_tracking = True,\n",
    "              #  logger=None,\n",
    "               )\n",
    "\n",
    "best_model = fc_session.train_and_evaluate(\n",
    "    # model = None, # (optional) pass a model to freeze/unfreeze, check, or change anything between runs\n",
    "    epochs = 10,\n",
    "    learning_rate = 1e-6,\n",
    "    best_metric = 0.,\n",
    "    best_model_dest = \"artifacts/MHIST_SmallFC_v1.pt\",  # DEFAULT_MODEL_PATH = 'MHIST_model.pt'\n",
    "    mlflow_experiment = \"MHIST FC (binary classification)\",\n",
    "    #  mlflow_run = None # create a new run\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167c616",
   "metadata": {},
   "source": [
    "### Train ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77665b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval on: improved true_positive_rate, ideally > 0.95 (fnr = 0.05)\n",
    "# Previous best: true_positive_rate = 0.9\n",
    "# Improved validation recall: 0.5482954545454546\n",
    "session = TrainingSession(\n",
    "    # train_transform=None,\n",
    "    # val_transform=None,\n",
    "    # path_for_resuming = 'artifacts/MHIST_FCN_run2', # resume from local path if not None (not used for saving best model)\n",
    "    # resume_from_object = False, # don't resume from model object\n",
    "    # model = best_model, # if not resuming, new FC model is initialized to random\n",
    "    model_type='VIT',\n",
    "    batch_size = 512,\n",
    "    eval_on='true_positive_rate', # for positive (minority) class\n",
    "    enable_tracking = True,\n",
    "    # logger=None,\n",
    "    )\n",
    "\n",
    "best_ViT = session.train_and_evaluate(\n",
    "    # model = None, # (optional) pass a model to freeze/unfreeze, check, or change anything between runs\n",
    "    epochs = 8,\n",
    "    # freeze_all_but_head = True,\n",
    "    learning_rate = 1e-2,\n",
    "    best_metric = 0.,\n",
    "    best_model_dest = \"artifacts/MHIST_ViT_v14.pt\",  # DEFAULT_MODEL_PATH = 'MHIST_model.pt'\n",
    "    mlflow_experiment = \"MHIST ViT (binary classification)\",\n",
    "    #  mlflow_run = None # create a new run\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a648d",
   "metadata": {},
   "source": [
    "### Run batch inference on best model\n",
    "After training, I ran batch inference on the test set with the best ViT model to inspect the results more carefully, checking for inaccurate classification and comparing the expert labeling with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = TrainingSession(\n",
    "    # train_transform=None,\n",
    "    # val_transform=None,\n",
    "    path_for_resuming = 'artifacts/MHIST_ViT_v13.pt', # resume from local path if not None (not used for saving best model)\n",
    "    # resume_from_object = False, # resume from model object\n",
    "    # model = None, # if not resume_from_object, model is initialized to random\n",
    "    model_type = 'VIT',\n",
    "    batch_size = 960,\n",
    "    # eval_on='loss',\n",
    "    # enable_tracking=False,\n",
    "    # logger=None,\n",
    "    )\n",
    "results_df, metrics = session.batch_predict()\n",
    "\n",
    "# # Save\n",
    "# results_df.to_csv('artifacts/MHIST_ViT_v13_results.csv', index=False)\n",
    "# pd.Series(metrics).to_json('artifacts/MHIST_ViT_v13_metrics.json')\n",
    "\n",
    "# For samples that have a negative prediction (pred class HP),\n",
    "# correct the probability to be = 1 - positive_prob\n",
    "corrected_results_df = results_df.copy()\n",
    "corrected_results_df['prob'] = corrected_results_df['positive_prob']\n",
    "neg_pred_samples = corrected_results_df['prediction'] == 0\n",
    "corrected_results_df.loc[neg_pred_samples, 'prob'] = 1 - corrected_results_df.loc[neg_pred_samples, 'prob']\n",
    "\n",
    "# Info about false negatives\n",
    "incorrect_df = corrected_results_df[(corrected_results_df['correct'] == 0) & (corrected_results_df['prob']>0.5) & (corrected_results_df['label']==1)]\n",
    "incorrect_annotations = incorrect_df.merge(annotations, left_on='filename', right_on='Image Name', how='left')\n",
    "cols = ['filename', 'label', 'prediction', 'positive_prob', 'prob', 'Number of Annotators who Selected SSA (Out of 7)']\n",
    "expert_comparison_df = incorrect_annotations.loc[incorrect_annotations['Number of Annotators who Selected SSA (Out of 7)']>5, cols]\n",
    "\n",
    "# Display model results and false negative info\n",
    "metrics, corrected_results_df.info(), corrected_results_df.head(), incorrect_annotations[cols], expert_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics on different thresholds for a given dataset\n",
    "# The dataframe is hard-coded in RESULTS_DF, below\n",
    "# Code is from Perplexity.ai\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, average_precision_score, balanced_accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULTS_DF = corrected_results_df\n",
    "\n",
    "def evaluate_threshold(y, y_prob, threshold):\n",
    "    y_true = y.astype(int)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fnr = fn / (fn + tp)\n",
    "    tpr = tp / (tp + fn)\n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'weighted_precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'weighted_recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'weighted_f1': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
    "        'pr_auc': average_precision_score(y_true, y_prob),\n",
    "        'false_negative_rate': fnr,\n",
    "        'true_positive_rate': tpr\n",
    "    }\n",
    "\n",
    "def find_best_thresholds(df):\n",
    "    y_true = df['label'].values\n",
    "    y_prob = df['positive_prob'].values\n",
    "\n",
    "    thresholds = np.arange(0.01, 1.00, 0.01)  # Expanded range\n",
    "    metrics = []\n",
    "\n",
    "    for threshold in tqdm(thresholds, desc=\"Evaluating thresholds\"):\n",
    "        metrics.append(evaluate_threshold(y_true, y_prob, threshold))\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    best_thresholds = {\n",
    "        'weighted_precision': metrics_df.loc[metrics_df['weighted_precision'].idxmax()],\n",
    "        'weighted_recall': metrics_df.loc[metrics_df['weighted_recall'].idxmax()],\n",
    "        'weighted_f1': metrics_df.loc[metrics_df['weighted_f1'].idxmax()],\n",
    "        'accuracy': metrics_df.loc[metrics_df['accuracy'].idxmax()],\n",
    "        'balanced_accuracy': metrics_df.loc[metrics_df['balanced_accuracy'].idxmax()],\n",
    "        'roc_auc': metrics_df.loc[metrics_df['roc_auc'].idxmax()],\n",
    "        'pr_auc': metrics_df.loc[metrics_df['pr_auc'].idxmax()],\n",
    "        'false_negative_rate': metrics_df.loc[metrics_df['false_negative_rate'].idxmin()]\n",
    "    }\n",
    "\n",
    "    return best_thresholds, metrics_df\n",
    "\n",
    "# Run the analysis on the cleaned data\n",
    "best_thresholds, metrics_df = find_best_thresholds(RESULTS_DF)\n",
    "\n",
    "# Print the results\n",
    "for metric, row in best_thresholds.items():\n",
    "    print(f\"\\nBest threshold for {metric}:\")\n",
    "    print(f\"Threshold: {row['threshold']:.2f}\")\n",
    "    print(f\"Weighted Precision: {row['weighted_precision']:.4f}\")\n",
    "    print(f\"Weighted Recall: {row['weighted_recall']:.4f}\")\n",
    "    print(f\"Weighted F1: {row['weighted_f1']:.4f}\")\n",
    "    print(f\"Accuracy: {row['accuracy']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {row['balanced_accuracy']:.4f}\")\n",
    "    print(f\"ROC AUC: {row['roc_auc']:.4f}\")\n",
    "    print(f\"PR AUC: {row['pr_auc']:.4f}\")\n",
    "    print(f\"False Negative Rate: {row['false_negative_rate']:.4f}\")\n",
    "    print(f\"True Positive Rate: {row['true_positive_rate']:.4f}\")\n",
    "\n",
    "# Plot the metrics across thresholds\n",
    "plt.figure(figsize=(12, 8))\n",
    "for column in metrics_df.columns:\n",
    "    if column != 'threshold':\n",
    "        plt.plot(metrics_df['threshold'], metrics_df[column], label=column)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Metrics vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot focusing on False Negative Rate and True Positive Rate\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(metrics_df['threshold'], metrics_df['false_negative_rate'], label='False Negative Rate')\n",
    "plt.plot(metrics_df['threshold'], metrics_df['true_positive_rate'], label='True Positive Rate')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('False Negative Rate and True Positive Rate vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the threshold that minimizes False Negative Rate\n",
    "min_fnr_threshold = metrics_df.loc[metrics_df['false_negative_rate'].idxmin(), 'threshold']\n",
    "print(f\"\\nThreshold that minimizes False Negative Rate: {min_fnr_threshold:.2f}\")\n",
    "print(\"Metrics at this threshold:\")\n",
    "print(metrics_df.loc[metrics_df['threshold'] == min_fnr_threshold].iloc[0].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics for a new threshold for a given dataset\n",
    "# Set RESULTS_DF and THRESHOLD below\n",
    "# Code is from Perplexity.ai\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, balanced_accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "RESULTS_DF = corrected_results_df\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "# Create the new DataFrame with adjusted predictions\n",
    "adjusted_threshold_df = results_df.copy()\n",
    "adjusted_threshold_df['prediction'] = (adjusted_threshold_df['positive_prob'] >= THRESHOLD).astype(int)\n",
    "adjusted_threshold_df['correct'] = (adjusted_threshold_df['prediction'] == adjusted_threshold_df['label']).astype(int)\n",
    "adjusted_threshold_df['prob'] = np.where(adjusted_threshold_df['prediction'] == 0,\n",
    "                                         1 - adjusted_threshold_df['positive_prob'],\n",
    "                                         adjusted_threshold_df['positive_prob'])\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = adjusted_threshold_df['label']\n",
    "y_pred = adjusted_threshold_df['prediction']\n",
    "y_prob = adjusted_threshold_df['positive_prob']\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "metrics = {\n",
    "    'Weighted Precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "    'weighted_recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "    'False Negative Rate': fn / (fn + tp),\n",
    "    'True Positive Rate': tp / (tp + fn),\n",
    "    'Weighted F1-Score': f1_score(y_true, y_pred, average='weighted'),\n",
    "    'Accuracy': accuracy_score(y_true, y_pred),\n",
    "    'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "    'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "    'PR-AUC': average_precision_score(y_true, y_prob)\n",
    "}\n",
    "\n",
    "# # Save the adjusted DataFrame to CSV\n",
    "# adjusted_threshold_df.to_csv('artifacts/MHIST_ViT_v13_adjusted_threshold_results.csv', index=False)\n",
    "\n",
    "# # Save metrics to a separate file\n",
    "# pd.Series(metrics).to_json('artifacts/MHIST_ViT_v13_adjusted_threshold_metrics.json')\n",
    "\n",
    "# Show metrics and new results\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "adjusted_threshold_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea8311",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "PyTorch does a lot of the work of converting models to ONNX. The new Dynamo export method integrates with ONNX to make conversion simple as well as efficient and accurate. I only selectively log models to MLflow to avoid uneccessary fees from AWS for data transfer and storage, which can add up quickly with large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ViT inference with the best model (PyTorch):\n",
    "!pip install -q timm # CPU-only version: pip install timm[torch-cpu]\n",
    "import timm\n",
    "vit = timm.create_model(\n",
    "    model_name='vit_base_patch16_224',\n",
    "    pretrained=False,\n",
    "    num_classes=1, # change number of outputs in classification head\n",
    "    # img_size=224\n",
    ")\n",
    "print('vit.head:\\n', vit.head) # Linear(in_features=768, out_features=1, bias=True)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "PYTORCH_MODEL_PATH = 'artifacts/MHIST_ViT_v13.pt'\n",
    "IMAGES_DIR = 'images/'\n",
    "PATH = 'MHIST_aah.png'\n",
    "test_df = pd.read_csv('artifacts/testset_info.csv')\n",
    "\n",
    "label = 'SSA' if test_df.loc[test_df['name'] == PATH, 'label'].item() == 1 else 'HP'\n",
    "\n",
    "image_path = os.path.join(IMAGES_DIR, PATH)\n",
    "image_PIL = Image.open(image_path).convert('RGB') # PIL Image size (224, 224)\n",
    "print('\\nimage_PIL dimensions', image_PIL.size)\n",
    "\n",
    "# Mean and std values were calculated from the training data, to normalize the colors (per channel):\n",
    "# Model expects the shape to be [BATCH, 3, 224, 224]\n",
    "TRAIN_MEAN = [0.738, 0.649, 0.775]\n",
    "TRAIN_STD =  [0.197, 0.244, 0.17]\n",
    "MHIST_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(TRAIN_MEAN, TRAIN_STD),\n",
    "])\n",
    "\n",
    "preprocessed_image = MHIST_transforms(image_PIL)\n",
    "print('preprocessed_image shape', preprocessed_image.shape)\n",
    "\n",
    "# Load model weights from PyTorch model checkpoint\n",
    "vit.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location=torch.device('cpu'))) # load model from disk, specify the location for mapping (loading) the model's params\n",
    "vit.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logit = vit(preprocessed_image.unsqueeze(0)) # torch.Size([1, 3, 224, 224]) with dtype = torch.float32\n",
    "    print('4D image.shape', preprocessed_image.unsqueeze(0).shape)\n",
    "    print('\\nlogit =', logit.item())\n",
    "    pred = logit.item() > 0 # Python bool\n",
    "    prob = torch.sigmoid(logit).item()\n",
    "    print('pred =', 'SSA' if pred else 'HP')\n",
    "    print('probability', prob if pred else 1-prob)\n",
    "print('label =', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ceebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export PyTorch model to ONNX\n",
    "!pip install --upgrade onnx onnxscript\n",
    "import torch.onnx\n",
    "\n",
    "# Generate an example input (image)\n",
    "example_input = torch.randn(224, 224, 3) # Tensor size (224, 224, 3)\n",
    "example_preprocessed = MHIST_transforms(example_input.numpy()) # includes ToTensor\n",
    "print('example_preprocessed shape', example_preprocessed.shape) # torch.Size([3, 224, 224])\n",
    "\n",
    "# Export to ONNX (with Dynamo)\n",
    "onnx_program = torch.onnx.dynamo_export(vit, example_preprocessed.unsqueeze(0))\n",
    "onnx_program.save(\"artifacts/MHIST_ViT_v13_dynamo_model.onnx\")\n",
    "onnx_program.model_proto.graph.input[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471614d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with local (not EFS) ONNX Dynamo model and S3\n",
    "# !pip install torch onnx\n",
    "!pip install -q onnxruntime\n",
    "import time\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# PYTORCH_MODEL_PATH = 'artifacts/MHIST_ViT_v13.pt'\n",
    "# IMAGES_DIR = 'images/'\n",
    "# PATH = 'MHIST_aah.png'\n",
    "\n",
    "import boto3\n",
    "S3_BUCKET = \"mhist-streamlit-app\"\n",
    "S3_ORIGINALS_DIR = \"images/original/\"\n",
    "image_filename = 'MHIST_aah.png'\n",
    "\n",
    "# For inference\n",
    "from onnxruntime import InferenceSession\n",
    "EFS_ACCESS_POINT = os.getcwd()\n",
    "MODEL_PATH = \"artifacts/MHIST_ViT_v13_dynamo_model.onnx\"\n",
    "\n",
    "def standardize_image(np_image):\n",
    "    # Convert lists to numpy\n",
    "    np_mean = np.array(TRAIN_MEAN, dtype=np.float32).reshape(3, 1, 1) # np_mean.shape (3, 1, 1)\n",
    "    np_std = np.array(TRAIN_STD, dtype=np.float32).reshape(3, 1, 1) # np_std.shape (3, 1, 1)\n",
    "\n",
    "    # Normalize: operations are performed element-wise using NumPy broadcasting\n",
    "    np_image = (np_image - np_mean) / np_std\n",
    "    return np_image\n",
    "\n",
    "\n",
    "# Images are normalized to range [0., 1.] and standardized by channel\n",
    "def preprocess(image_filename):\n",
    "    # Download image (png file) as bytes from S3\n",
    "    image_s3key = os.path.join(S3_ORIGINALS_DIR, image_filename)\n",
    "    s3 = boto3.client('s3')\n",
    "    file_obj = s3.get_object(Bucket=S3_BUCKET, Key=image_s3key)\n",
    "    image_bytes = BytesIO(file_obj['Body'].read())\n",
    "\n",
    "    # Convert bytes (buffer) to 3-channels, then to ndarray\n",
    "    # We could do this without PIL (using only NumPy)\n",
    "    pil_image = Image.open(image_bytes).convert('RGB') # pil_image.size (224, 224) with 3 channels\n",
    "    np_image = np.array(pil_image, dtype=np.float32) # np_image shape (224, 224, 3) dtype float32\n",
    "    transposed_np = np.transpose(np_image, (2, 0, 1)) # shape (3, 224, 224) max pixel value = 255.\n",
    "    normalized_np = transposed_np / 255.0 # normalize range to [0., 1.]\n",
    "    standardized_np = standardize_image(normalized_np) # normalize color-channels\n",
    "    return np.expand_dims(standardized_np, axis=0)\n",
    "\n",
    "\n",
    "def sigmoid(np_outs):\n",
    "    np_outs = np.clip(np_outs, -50, 50) # prevent np.exp overflow for large values\n",
    "    return 1 / (1 + np.exp(-np_outs))\n",
    "\n",
    "\n",
    "def predict(image_filename): # image_url <class '_io.BytesIO'>\n",
    "    print('EFS_ACCESS_POINT contents:', os.listdir(EFS_ACCESS_POINT))\n",
    "\n",
    "    # Run inference with optimized ONNX model\n",
    "    # It only uses 3.3 GB CPU memory, and 1.4 GB space (for artifacts)\n",
    "    onnx_path = os.path.join(EFS_ACCESS_POINT, MODEL_PATH)\n",
    "    ort_session = InferenceSession(os.path.abspath(onnx_path))#, providers=['CPUExecutionProvider'])\n",
    "\n",
    "    start_inference = time.monotonic()\n",
    "    preprocessed_image = preprocess(image_filename)\n",
    "    preprocess_time = time.monotonic()\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    ort_outs = ort_session.run(None, {input_name: preprocessed_image}) # output: [array([-1.2028292], dtype=float32)]\n",
    "    inference_time = time.monotonic()\n",
    "\n",
    "    logit = ort_outs[0].item() # <class 'numpy.ndarray'> shape (1,) dtype=float32\n",
    "    positive_prob = sigmoid(logit).item()\n",
    "    pred = positive_prob > 0.3\n",
    "    inference_info = {#json.dumps({\n",
    "        'logit': logit,\n",
    "        'predicted_class': 'SSA' if pred else 'HP',\n",
    "        'probability': positive_prob if pred else 1-positive_prob,\n",
    "        'preprocess_time': preprocess_time-start_inference,\n",
    "        'inference_time': inference_time-preprocess_time,\n",
    "        }\n",
    "    return inference_info\n",
    "\n",
    "\n",
    "post_start = time.monotonic()\n",
    "r_dict = predict(image_filename)\n",
    "onnx_runtime = time.monotonic() - post_start\n",
    "\n",
    "print('Completed inference on image_filename', image_filename, 'logit', r_dict['logit'])\n",
    "print('inference_info:\\n', r_dict)\n",
    "\n",
    "correct = r_dict['predicted_class'] == 'HP'\n",
    "class_type = 'positive' if r_dict['predicted_class'] == 'SSA' else 'negative'\n",
    "print(f\"Prediction: {r_dict['predicted_class']}, which is a {str(correct).lower()} {class_type}\")\n",
    "print(f\"Model's predicted probability: {r_dict['probability']*100:.2f}%\")\n",
    "print(f\"Preprocessed image in {r_dict['preprocess_time']:.2f} seconds\")\n",
    "print(f\"Classified image in {r_dict['inference_time']:.2f} seconds\")\n",
    "print(f\"Total: {int(round(onnx_runtime))} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log best ViT model (as a generic artifact) to MLflow server\n",
    "# import mlflow\n",
    "print('Using:', 'boto3', boto3.__version__, 'mlflow', mlflow.__version__)#, 'onnx', onnx.__version__)\n",
    "\n",
    "MLFLOW_SERVER=\"http://13.52.243.246:5000\"\n",
    "MLFLOW_EXPERIMENT = 'MHIST ViT (binary classification)'\n",
    "DYNAMO_MODEL_PATH = \"artifacts/MHIST_ViT_v13_dynamo_model.onnx\"\n",
    "MLFLOW_MODEL_PATH = 'onnx_artifacts'\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER)\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "run_id = '84074e5ab58749f1b609ef5ef90c499f'\n",
    "# run_name = masked-sheep-165\n",
    "\n",
    "# with mlflow.start_run(run_id) as run: #any active run will be ended\n",
    "#     print('\\nLogging metrics and best model with MLflow: run_name =', run.info.run_name, 'run_id =', run.info.run_id) # run_id is a UUID\n",
    "#     print('MLflow server:', mlflow.get_tracking_uri())\n",
    "#     mlflow.log_artifact(DYNAMO_MODEL_PATH, artifact_path=MLFLOW_MODEL_PATH)\n",
    "#     # mlflow.log_artifact(local_path, artifact_path=None)\n",
    "#     # mlflow.log_artifacts(local_dir, artifact_path=None)\n",
    "#     print('Artifacts stored at:', mlflow.get_artifact_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51179bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log best ViT model (as a generic artifact) to MLflow server\n",
    "# import mlflow\n",
    "print('Using:', 'boto3', boto3.__version__, 'mlflow', mlflow.__version__)#, 'onnx', onnx.__version__)\n",
    "\n",
    "MLFLOW_SERVER=\"http://13.52.243.246:5000\"\n",
    "MLFLOW_EXPERIMENT = 'MHIST ViT (binary classification)'\n",
    "DYNAMO_MODEL_PATH = \"artifacts/MHIST_ViT_v13_dynamo_model.onnx\"\n",
    "MLFLOW_MODEL_PATH = 'onnx_artifacts'\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER)\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "run_id = '84074e5ab58749f1b609ef5ef90c499f'\n",
    "# run_name = masked-sheep-165\n",
    "\n",
    "with mlflow.start_run(run_id) as run: #any active run will be ended\n",
    "    print('\\nLogging metrics and best model with MLflow: run_name =', run.info.run_name, 'run_id =', run.info.run_id) # run_id is a UUID\n",
    "    print('MLflow server:', mlflow.get_tracking_uri())\n",
    "    mlflow.log_artifact(DYNAMO_MODEL_PATH, artifact_path=MLFLOW_MODEL_PATH)\n",
    "    print('Artifacts stored at:', mlflow.get_artifact_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b746baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
