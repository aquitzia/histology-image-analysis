{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb6fb2c-9c71-40a9-88e8-e36c5f0dded1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ec2-user/SageMaker/histology-image-analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "IMAGES_DIR = os.environ['SM_CHANNEL_TRAINING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1ce05-89ef-40a6-b527-e0fb3a5ba093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Perplexity.ai\n",
    "# When creating your PyTorch estimator,\n",
    "# specify the S3 location of your data as an input channel:\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='m5.xlarge',\n",
    "    # framework_version='1.8.1',\n",
    "    # py_version='py3',\n",
    "    input_mode='File',\n",
    "    inputs={'training': 's3://mhist-streamlit-app/images/original/'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5d59a5-408f-49f1-9fa9-c3073eba1d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape (2175, 3)\n",
      "test_df.shape (977, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(            name  experts  label\n",
       " 0  MHIST_aaa.png        6      1\n",
       " 1  MHIST_aab.png        0      0\n",
       " 2  MHIST_aac.png        5      1\n",
       " 3  MHIST_aae.png        1      0\n",
       " 4  MHIST_aaf.png        5      1,\n",
       "             name  experts  label\n",
       " 0  MHIST_aag.png        2      0\n",
       " 1  MHIST_aah.png        2      0\n",
       " 2  MHIST_aaq.png        5      1\n",
       " 3  MHIST_aar.png        0      0\n",
       " 4  MHIST_aay.png        1      0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV files\n",
    "import pandas as pd\n",
    "\n",
    "# image codes are 3 letters long\n",
    "# 'name' : MHIST_<code>.png\n",
    "# 'label' = HP or SSA # binary, categorical label\n",
    "# 'experts' = (int) 0 through 7\n",
    "# 'code' = 3-letter image code\n",
    "\n",
    "# Training set samples: 2175\n",
    "# Test set samples: 977\n",
    "train_df = pd.read_csv('training/trainset_info.csv')\n",
    "test_df = pd.read_csv('training/testset_info.csv')\n",
    "print('train_df.shape', train_df.shape)\n",
    "print('test_df.shape', test_df.shape)\n",
    "train_df.head(), test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c755412e-4fc5-4535-aac6-05e90eaec409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Don't resize nor crop. These are medical images, so we don't want to lose\n",
    "# image integrity. Also, most models, like ViT expect images to be 224x224 pixels.\n",
    "\n",
    "# ToTensor: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a\n",
    "# torch.FloatTensor with shape (C x H x W) in the range [0.0, 1.0]\n",
    "\n",
    "# For Normalize: (calculated from the training data per channel)\n",
    "train_mean = [0.738, 0.649, 0.775]\n",
    "train_std =  [0.197, 0.244, 0.17]\n",
    "\n",
    "# Flatten data for FC\n",
    "DEFAULT_FC_TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "\n",
    "# Don't need to flatten our 2-D, 3-channel image data for ViT\n",
    "DEFAULT_VIT_TRANSFORMS = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "])\n",
    "\n",
    "# We don't need to use a different transform for test sets here because\n",
    "# we're only pre-processing images, not adding synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6859ff-90a5-4f4e-8232-9b3c0ea9e68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset class:\n",
    "class MHIST_dataset(Dataset):\n",
    "    def __init__(self, df, images_dir=IMAGES_DIR, transform=None):\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        if transform == None:\n",
    "            print(\"Error: missing transform for MHIST_dataset\")\n",
    "            raise ValueError(\"Error: missing transform for MHIST_dataset\")\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    # getitem() returns {image tensor, label int64, image filename}\n",
    "    def __getitem__(self, idx):\n",
    "        # df['label'] = 0 or 1 (int64)\n",
    "        # df['name'] ex: MHIST_abc.png\n",
    "        row = self.df.iloc[idx]\n",
    "        full_path = os.path.join(self.images_dir, row['name'])\n",
    "        image_PIL = Image.open(full_path).convert('RGB')\n",
    "        if image_PIL is None:\n",
    "            raise FileNotFoundError(full_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image_PIL) # includes ToTensor\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': row['label'],\n",
    "            'filename': row['name'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e471661-bfbc-447d-b071-9040902f216e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedRandomSampler len: 2175 dtype float64\n"
     ]
    }
   ],
   "source": [
    "# Create a WeightedRandomSampler to balance the training data (not validation data)\n",
    "# This will randomly oversample the minority class and undersample majority class during training\n",
    "import numpy as np\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e898262-085f-47b1-b74d-d5a4557c11d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32\n",
      "Floor of labels/batch size from annotations:  67 balanced train batches,  30  val batches\n",
      "                    Batches from dataloader:  68 balanced train batches,  31  val batches\n"
     ]
    }
   ],
   "source": [
    "# Use the random sampler instead of shuffle to create DataLoaders\n",
    "# Use the original Dataset and Dataloader for validation\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32 #256 for A100\n",
    "SHUFFLE = True\n",
    "DROP_LAST_BATCH = False # The dataset size might not divisible by the batch size\n",
    "\n",
    "TRAIN_SAMPLES = 2175\n",
    "TEST_SAMPLES  =  977\n",
    "\n",
    "over_FC_train_loader = DataLoader(MHIST_dataset(train_df, transform=DEFAULT_FC_TRANSFORMS),\n",
    "                                    batch_size=BATCH_SIZE, sampler=sampler, drop_last=DROP_LAST_BATCH)\n",
    "\n",
    "FC_val_loader = DataLoader(MHIST_dataset(test_df, transform=DEFAULT_FC_TRANSFORMS),\n",
    "                        batch_size=BATCH_SIZE, shuffle=SHUFFLE, drop_last=DROP_LAST_BATCH)\n",
    "\n",
    "over_FC_loaders = {'train':over_FC_train_loader, 'val':FC_val_loader}\n",
    "\n",
    "num_batches     = math.floor(TRAIN_SAMPLES/BATCH_SIZE)\n",
    "val_num_batches = math.floor(TEST_SAMPLES/BATCH_SIZE)\n",
    "print('batch_size', BATCH_SIZE)\n",
    "print('Floor of labels/batch size from annotations: ', num_batches, 'balanced train batches, ', val_num_batches, ' val batches')\n",
    "print('                    Batches from dataloader: ', len(over_FC_loaders['train']), 'balanced train batches, ', len(over_FC_loaders['val']), ' val batches')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225e210-bf6a-4a28-a325-46a70af7fe1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
