### **Vision Transformer**

This model is based on ViT, which stands for Vision Transformer. It was designed by Google in 2020 and it was pre-trained on images that are 224 x 224 pixels. Before transformers became popular, CNN (convolutional neural network) models set the standard in computer vision. We have, now, adapted transformers (similar to ChatGPT), to be used effectively on images as well as text. ViT breaks up an image into patches, which are 16x16 pixels. These patches are processed linearly and treated as a sequence of tokens, similar to NLP transformers.

**Fine-Tuning**
I used the timm library, rather than Huggingface, to download the pretrained model and easily add a classification head for this (binary) classification task, before fine-tuning the model on a publicly available dataset: MHIST. The data is quite imbalanced due to the nature of cancer screenings. There are many more negative than positive samples, which makes training more complex. For this dataset, F1-score should have been a great metric to guide training because it is a more balanced measure than accuracy or other standard metrics. However, it was necessary to focus more on the true-positive rate while training on this dataset to compel the model to focus on false negatives, in particular. A false negative can be quite detrimental in pathology because it would give a patient a false sense of security. A patient that has a negative result would be treated as quickly, possibly leading to dire consequences.

However, when I focused the training on loss, while saving models on an evaluation metric of true-positive rate, the model became overly focused on true positive samples, quickly overfitting and the F1-score, ROC-AUC, and other balanced metrics were quite low. Weighted and balanced metrics were often near 50/50 chance, which is close to random. As expected, it was important to train on multiple metrics simultaneously. It was helpful for me to create my own method of training that keeps various cases in mind so that, as TPR increased, it wouldn't excessively skew training or prevent an undesirable drop in the balanced metrics.

I also needed to adjust the threshold to squeeze the most out of training and inference on this imbalanced dataset with a transformer that can't "see" the same way a CNN can. I was quite pleased to find out that adjusting my threshold from the typical 0.5 to 0.4 and sometimes as low as 0.3 would significantly increase TPR and some of the balanced metrics. For example, ROC-AUC increased from 0.5 to 0.8764. Of course, I could have decreased it even more to get 100% TPR, but that negatively impacts other metrics.

**Training**
I created a custom trainer object (in Pytorch) to suit my purposes and ran several experiments. When I trained all weights with a large learning rate, I quickly lost the benefit of using pre-trained weights and the model overfit quickly.

I had much better results when I froze all weights except the classification head, then lowered the learning rate and fine-tuned all parameters. I was surprised to find that a high learning rate was important for training this model, as well as using a very focused metric, true-positive rate, as described above. I ran enough training epochs with the training set such that the loss and evaluation loss decreased as much as possible before evaluation loss began to increase. As the training loss approached zero, the evaluation loss worsened, as expected, assuring me that the model was able to get the most out of the training with the hyperparameters and methods that I used. I found that learning rates as high as 0.01 to 0.1 gave me the best results.