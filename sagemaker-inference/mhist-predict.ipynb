{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bdc037",
   "metadata": {},
   "source": [
    "# **Deploy a pretrained, optimized ONNX model to SageMaker Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2e70d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/aquitzia/histology-image-analysis\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "# Git doesn't work well within the AWS Studio Code Editor space\n",
    "# Make sure the code is up-to-date:\n",
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a1692c-cfa0-4bf1-bbd6-f607e54d3746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/histology-image-analysis/sagemaker-inference\n",
      "mhist-predict.ipynb  model.tar.gz  src\ttest_locally.py\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls\n",
    "\n",
    "# SageMaker Studio Code Editor working directory:\n",
    "# /home/sagemaker-user/histology-image-analysis/sagemaker-inference\n",
    "\n",
    "# SageMaker Notebook Instance working directory:\n",
    "# /home/ec2-user/SageMaker/histology-image-analysis/sagemaker-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f1047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHIST_ViT_v13_dynamo_model.onnx\n",
      "src/\n",
      "src/inference.py\n",
      "src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# # Download from s3 and uncompress\n",
    "# import os\n",
    "# import boto3\n",
    "\n",
    "# s3 = boto3.client('s3')\n",
    "# s3.download_file(Bucket='sagemaker-us-west-1-851725529671', Key='mhist-vit-model/model.tar.gz', Filename='model.tar.gz')\n",
    "\n",
    "# # tar:\n",
    "# # -x extract\n",
    "# # -z gzip\n",
    "# # -v verbose\n",
    "# # -f from filename\n",
    "# !tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17726b8",
   "metadata": {},
   "source": [
    "### Test and Upload model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b4d40",
   "metadata": {},
   "source": [
    "SageMaker recommends the structure:\n",
    "```\n",
    "model.tar.gz/\n",
    "|- model.pth\n",
    "|- src/\n",
    "  |- inference.py\n",
    "  |- requirements.txt  # only for versions 1.3.1 and higher\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb50747d-3c7a-417a-a8cb-9a250d4e0056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_type application/json\n",
      "Output: ('{\"logit\": 3.948834180831909, \"predicted_class\": \"SSA\", \"probability\": 0.9810874185378591}', 'application/json')\n"
     ]
    }
   ],
   "source": [
    "# Test inference locally\n",
    "!pip install -U -q -r src/requirements.txt\n",
    "%run test_locally.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923236bc-9840-4f39-b510-5628992f15f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHIST_ViT_v13_dynamo_model.onnx\n",
      "src/\n",
      "src/inference.py\n",
      "src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Compress\n",
    "# -c create archive\n",
    "# -z gzip\n",
    "# -v verbose\n",
    "# -f to filename\n",
    "print('Archive contents:')\n",
    "!tar -czvf model.tar.gz MHIST_ViT_v13_dynamo_model.onnx src\n",
    "print('\\nArchive info:')\n",
    "!ls -lha model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d69bc79-b266-4c53-8230-be6f40fb2892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files uploaded to: s3://mhist-artifacts-2024/sagemaker-model-artifacts/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload model artifacts to S3\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "MODEL_BUCKET = 'mhist-artifacts-2024'\n",
    "S3_PREFIX = 'sagemaker-model-artifacts'\n",
    "S3_FILENAME = 'model.tar.gz'\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "model_artifacts_archive = sagemaker_session.upload_data(\n",
    "    path=S3_FILENAME,\n",
    "    bucket=MODEL_BUCKET,\n",
    "    key_prefix=S3_PREFIX)\n",
    "\n",
    "print(f\"Model files uploaded to: {model_artifacts_archive}\")\n",
    "# Model files uploaded to: s3://mhist-artifacts-2024/sagemaker-model-artifacts/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd3bfa",
   "metadata": {},
   "source": [
    "The output message states that SageMaker SDK is using its built-in default settings rather than any custom configurations, located at:\n",
    "- `/etc/xdg/sagemaker/config.yaml`: system-wide config\n",
    "- `/home/sagemaker-user/.config/sagemaker/config.yaml`: user-specific config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee153ca",
   "metadata": {},
   "source": [
    "### Deploy PyTorchModel server and PyTorchPredictor Endpoint:\n",
    "We will deploy a PyTorch model trained outside of SageMaker. The AWS PyTorchModel server is natively integrated with TorchServe, an open-source project developed by AWS and Facebook to serve PyTorch models.\n",
    "1. Set up a SageMaker Python SDK PyTorchModel object, set an entry_point\n",
    "2. Deploy the model to create a PyTorchPredictor, which manages a SageMaker Endpoint.\n",
    "A SageMaker Endpoint is a hosted prediction service for performing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ac6845f-d785-4339-8c8a-00261f2be6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the SageMaker PyTorchModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "# model_artifacts_archive = 's3://sagemaker-us-west-1-851725529671/mhist-vit/model.tar.gz'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    # Model params\n",
    "    model_data=model_artifacts_archive,\n",
    "    role=role,\n",
    "    source_dir='src',\n",
    "    entry_point='inference.py',\n",
    "\n",
    "    # PyTorchModel params\n",
    "    framework_version='2.3.0',\n",
    "    py_version='py311',\n",
    "    dependencies=['src/requirements.txt']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9c958ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = sagemaker_session.sagemaker_client\n",
    "role = sagemaker.get_execution_role() # arn:aws:iam::851725529671:role/SageMakerEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e62aeeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!\n",
      "Deployed PyTorchModel: pytorch-inference-2024-07-22-10-29-54-289\n",
      "Instance Recommendations:\n",
      "    - ml.g4dn.xlarge\n",
      "    - ml.g4dn.2xlarge\n",
      "    - ml.c6i.xlarge\n",
      "\n",
      "PyTorchPredictor Endpoint: pytorch-inference-2024-07-22-10-29-55-303\n",
      "Image pushed to ECR repo: 763104351884.dkr.ecr.us-west-1.amazonaws.com/pytorch-inference:2.3.0-cpu-py311\n",
      "\n",
      "Deleted Endpoint, Configuration, and Artifacts:\n",
      "pytorch-inference-2024-07-22-10-29-21-491/model.tar.gz | LastModified: 2024-07-22 10:29:51+00:00 | Size: 318973566\n",
      "\n",
      " Model output: {'logit': 3.948834180831909, 'predicted_class': 'SSA', 'probability': 0.9810874185378591}\n"
     ]
    }
   ],
   "source": [
    "# Create a SageMaker Model, Endpoint, and Endpoint Configuration\n",
    "\n",
    "predictor = model.deploy( # returns a PyTorchPredictor\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    initial_instance_count=1,\n",
    "    serializer=JSONSerializer(), # Default serializes input data to .npy format\n",
    "    deserializer=JSONDeserializer() # Default parses the response from .npy format to numpy array.\n",
    ")\n",
    "# print(f\"\\nDeployed PyTorchModel: {model.name}\")\n",
    "model_info = sagemaker_client.describe_model(ModelName=model.name)\n",
    "print('\\nDeployed PyTorchModel:', model_info['ModelName'])\n",
    "print('Instance Recommendations:')\n",
    "recs = model_info['DeploymentRecommendation']['RealTimeInferenceRecommendations']\n",
    "for rec in recs:\n",
    "    print('    -', rec['InstanceType'])\n",
    "\n",
    "print(f\"\\nPyTorchPredictor Endpoint: {predictor.endpoint_name}\")\n",
    "endpoint_info = sagemaker_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "for prod in endpoint_info['ProductionVariants']:\n",
    "    for deployed in prod['DeployedImages']:\n",
    "        print('Image pushed to ECR repo:', deployed['SpecifiedImage'])\n",
    "\n",
    "# Endpoint Cofiguration name matches Endpoint name\n",
    "# print(f\"PyTorchPredictor Endpoint Configuration: {endpoint_info['EndpointConfigName']}\")\n",
    "\n",
    "# Use PyTorchPredictor to run inference on an Endpoint (instance)\n",
    "# Predictor's default is to serialize Python lists, dictionaries, and numpy arrays\n",
    "# to multidimensional tensors for PyTorch inference.\n",
    "# Here, we pass the image path(s) for the computer vision model\n",
    "response = predictor.predict({\n",
    "    'bucket': 'mhist-streamlit-app',\n",
    "    'key': 'images/original/MHIST_aah.png'\n",
    "})\n",
    "\n",
    "# Delete Endpoint, which incurs significant fees to run\n",
    "predictor.delete_endpoint()\n",
    "print(f\"\\nDeleted Endpoint, Configuration, and Artifacts:\")\n",
    "\n",
    "# Delete all artifacts from sagemaker.Session().default_bucket():\n",
    "objects = s3.list_objects_v2(Bucket=sagemaker_session.default_bucket()) # 'sagemaker-us-west-1-851725529671'\n",
    "for obj in objects.get('Contents', []):\n",
    "    print(f\"{obj['Key']} | LastModified: {obj['LastModified']} | Size: {obj['Size']}\")\n",
    "    s3.delete_object(Bucket=bucket, Key=obj['Key'])\n",
    "\n",
    "# Expected output:\n",
    "# {\"logit\": 3.948834180831909,\n",
    "# \"predicted_class\": \"SSA\",\n",
    "# \"probability\": 0.9810874185378591}\n",
    "print('\\n Model output:', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e744d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output:\n",
    "\n",
    "# model artifact for PyTorchModel:\n",
    "# mhist-vit/model.tar.gz LastModified: 2024-07-22 03:12:54+00:00 Size: 318708924\n",
    "\n",
    "# model artifact for PyTorchModel Endpoint:\n",
    "# pytorch-inference-2024-07-22-03-13-07-428/model.tar.gz LastModified: 2024-07-22 03:13:34+00:00 Size: 318973586\n",
    "\n",
    "# s3.list_objects_v2 returns ResponseMetadata:\n",
    "# RequestId- same as x-amz-request-id (below)\n",
    "# HostId- host that responded (s3 id)\n",
    "# HTTPStatusCode- 200 for success\n",
    "# HTTPHeaders:\n",
    "#       x-amz-id-2: s3 id\n",
    "#       x-amz-request-id: AWS id for the request\n",
    "#       date\n",
    "#       x-amz-bucket-region\n",
    "#       content-type\n",
    "#       transfer-encoding: 'chunked' response\n",
    "#       server: 'AmazonS3'\n",
    "# RetryAttempts = 0\n",
    "# IsTruncated\n",
    "# Contents: (list of dicts)\n",
    "#       Key\n",
    "#       LastModified\n",
    "#       ETag\n",
    "#       Size\n",
    "# StorageClass = 'STANDARD'\n",
    "# Name = 'sagemaker-us-west-1-851725529671'\n",
    "# Prefix = ''\n",
    "# MaxKeys = 1000\n",
    "# EncodingType = url\n",
    "# KeyCount = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433f4dbf-26a0-455b-a5e8-ac8901070baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optionally delete the SageMaker model, which doesn't incur charges\n",
    "# # (or go to SageMaker Studio --> Models --> Deployable Models)\n",
    "# model.delete_model()\n",
    "\n",
    "# # If we delete the SageMaker model, delete the associated artifact in S3\n",
    "# import boto3\n",
    "# BUCKET = 'mhist-artifacts-2024'\n",
    "# S3_PREFIX = 'sagemaker-model-artifacts'\n",
    "# S3_FILENAME = 'model.tar.gz'\n",
    "# s3 = boto3.client('s3')\n",
    "# s3.delete_object(Bucket=BUCKET, Key=f\"{S3_PREFIX}/{S3_FILENAME}\")\n",
    "\n",
    "# # sagemaker.Session() object doesn't use any other resources (besides notebook memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbdad27",
   "metadata": {},
   "source": [
    "Also, remember to stop the Studio Instance:\n",
    "- Close this notebook, then click **SageMaker Studio --> Running Instances --> Stop**\n",
    "- When you stop the Studio instance, SageMaker with delete the associated EBS volume\n",
    "\n",
    "To double-check, go to the EC2 console\n",
    "- In the left sidebar, click Elastic Block Store --> Volumes\n",
    "- Look for any volumes with a name starting with \"sagemaker-\"\n",
    "\n",
    "Check **AWS Billing** dashboard to check for any resources that might be used accidentally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
