{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bdc037",
   "metadata": {},
   "source": [
    "# **Deploy a pretrained, optimized ONNX model to SageMaker Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2e70d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/aquitzia/histology-image-analysis\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "# Git doesn't work well within the AWS Studio Code Editor space\n",
    "# Make sure the code is up-to-date:\n",
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a1692c-cfa0-4bf1-bbd6-f607e54d3746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/histology-image-analysis/sagemaker-inference\n",
      "MHIST_ViT_v13_dynamo_model.onnx  model.tar.gz  test_locally.py\n",
      "mhist-predict.ipynb\t\t src\n"
     ]
    }
   ],
   "source": [
    "S3_PREFIX = 'mhist-vit'\n",
    "S3_FILENAME = 'model.tar.gz'\n",
    "\n",
    "!pwd\n",
    "!ls\n",
    "\n",
    "# SageMaker Studio Code Editor working directory:\n",
    "# /home/sagemaker-user/histology-image-analysis/sagemaker-inference\n",
    "\n",
    "# SageMaker Notebook Instance working directory:\n",
    "# /home/ec2-user/SageMaker/histology-image-analysis/sagemaker-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f1047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHIST_ViT_v13_dynamo_model.onnx\n",
      "src/\n",
      "src/inference.py\n",
      "src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# # Download from s3 and uncompress\n",
    "# import os\n",
    "# import boto3\n",
    "\n",
    "# s3 = boto3.client('s3')\n",
    "# s3.download_file(Bucket='sagemaker-us-west-1-851725529671', Key='mhist-vit-model/model.tar.gz', Filename='model.tar.gz')\n",
    "\n",
    "# # tar:\n",
    "# # -x extract\n",
    "# # -z gzip\n",
    "# # -v verbose\n",
    "# # -f from filename\n",
    "# !tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17726b8",
   "metadata": {},
   "source": [
    "### Test and Upload model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b4d40",
   "metadata": {},
   "source": [
    "SageMaker recommends the structure:\n",
    "```\n",
    "model.tar.gz/\n",
    "|- model.pth\n",
    "|- src/\n",
    "  |- inference.py\n",
    "  |- requirements.txt  # only for versions 1.3.1 and higher\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb50747d-3c7a-417a-a8cb-9a250d4e0056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_type application/json\n",
      "Output: ('{\"logit\": 3.948834180831909, \"predicted_class\": \"SSA\", \"probability\": 0.9810874185378591}', 'application/json')\n"
     ]
    }
   ],
   "source": [
    "# Test inference locally\n",
    "!pip install -U -q -r src/requirements.txt\n",
    "%run test_locally.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923236bc-9840-4f39-b510-5628992f15f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHIST_ViT_v13_dynamo_model.onnx\n",
      "src/\n",
      "src/inference.py\n",
      "src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Compress\n",
    "# -c create archive\n",
    "# -z gzip\n",
    "# -v verbose\n",
    "# -f to filename\n",
    "print('Archive contents:')\n",
    "!tar -czvf model.tar.gz MHIST_ViT_v13_dynamo_model.onnx src\n",
    "print('\\nArchive info:')\n",
    "!ls -lha model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d69bc79-b266-4c53-8230-be6f40fb2892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files uploaded to: s3://sagemaker-us-west-1-851725529671/mhist-vit/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload model artifacts to SageMaker default bucket\n",
    "# !pip install -U sagemaker\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket() # 'sagemaker-us-west-1-851725529671'\n",
    "S3_FILENAME = 'model.tar.gz'\n",
    "S3_PREFIX = 'mhist-vit'\n",
    "\n",
    "model_path = sagemaker_session.upload_data(\n",
    "    path=S3_FILENAME,\n",
    "    bucket=bucket,\n",
    "    key_prefix=S3_PREFIX)\n",
    "\n",
    "print(f\"Model files uploaded to: {model_path}\")\n",
    "# Model files uploaded to: s3://sagemaker-us-west-1-851725529671/mhist-vit-model/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd3bfa",
   "metadata": {},
   "source": [
    "The output message states that SageMaker SDK is using its built-in default settings rather than any custom configurations, located at:\n",
    "- `/etc/xdg/sagemaker/config.yaml`: system-wide config\n",
    "- `/home/sagemaker-user/.config/sagemaker/config.yaml`: user-specific config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all model artifacts in S3\n",
    "s3 = boto3.client('s3')\n",
    "objects = s3.list_objects_v2(Bucket=bucket)\n",
    "print(f\"{objects['KeyCount']} model artifacts remaining in S3 bucket: {objects['Name']}\")\n",
    "for obj in objects.get('Contents', []): # returns [] by default\n",
    "    print(f\"{obj['Key']} LastModified: {obj['LastModified']} Size: {obj['Size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee153ca",
   "metadata": {},
   "source": [
    "### Deploy PyTorchModel server and PyTorchPredictor Endpoint:\n",
    "- create a PyTorchModel object and set an entry_point\n",
    "- deploy a PyTorchPredictor\n",
    "\n",
    "This creates a SageMaker Endpoint -- a hosted prediction service that we can use to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ac6845f-d785-4339-8c8a-00261f2be6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!\n",
      "PyTorchModel: pytorch-inference-2024-07-22-03-19-10-318\n",
      "PyTorchPredictor Endpoint: pytorch-inference-2024-07-22-03-19-11-113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logit': 3.948834180831909,\n",
       " 'predicted_class': 'SSA',\n",
       " 'probability': 0.9810874185378591}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PyTorch Endpoint from SageMaker Python SDK's PyTorch Model\n",
    "# to deploy a PyTorch model trained outside of SageMaker.\n",
    "# The AWS Model server is natively integrated with TorchServe,\n",
    "# an open-source project developed by AWS and Facebook to serve PyTorch models.\n",
    "\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() # arn:aws:iam::851725529671:role/SageMakerEx\n",
    "# model_path = 's3://sagemaker-us-west-1-851725529671/mhist-vit/model.tar.gz'\n",
    "\n",
    "# Configure the SageMaker PyTorch model server\n",
    "model = PyTorchModel(\n",
    "    # Model params\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    source_dir='src',\n",
    "    entry_point='inference.py',\n",
    "\n",
    "    # PyTorchModel params\n",
    "    framework_version='2.3.0',\n",
    "    py_version='py311',\n",
    "    dependencies=['src/requirements.txt']\n",
    ")\n",
    "print(f\"\\nPyTorchModel: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all model artifacts in S3\n",
    "s3 = boto3.client('s3')\n",
    "objects = s3.list_objects_v2(Bucket=bucket)\n",
    "print(f\"{objects['KeyCount']} model artifacts remaining in S3 bucket: {objects['Name']}\")\n",
    "for obj in objects.get('Contents', []): # returns [] by default\n",
    "    print(f\"{obj['Key']} LastModified: {obj['LastModified']} Size: {obj['Size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62aeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy() creates a SageMaker Endpoint, a hosted prediction service\n",
    "# returns a PyTorchPredictor, which runs inference on PyTorch Endpoints\n",
    "# with the (above) PyTorch model server.\n",
    "# Predictor will serialize Python lists, dictionaries, and numpy arrays\n",
    "# to multidimensional tensors for PyTorch inference.\n",
    "predictor = model.deploy(\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    initial_instance_count=1,\n",
    "    serializer=JSONSerializer(), # Default serializes input data to .npy format\n",
    "    deserializer=JSONDeserializer() # Default parses the response from .npy format to numpy array.\n",
    ")\n",
    "print(f\"\\nPyTorchModel: {model.name}\")\n",
    "print(f\"PyTorchPredictor Endpoint: {predictor.endpoint_name}\")\n",
    "\n",
    "# Test Endpoint \n",
    "response = predictor.predict({\n",
    "    'bucket': 'mhist-streamlit-app',\n",
    "    'key': 'images/original/MHIST_aah.png'\n",
    "})\n",
    "\n",
    "# Expected output:\n",
    "# {\"logit\": 3.948834180831909,\n",
    "# \"predicted_class\": \"SSA\",\n",
    "# \"probability\": 0.9810874185378591}\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10bee1",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28798c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete SageMaker Endpoint, which incurs significant fees to run\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e744d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 model artifacts remaining in S3 bucket: sagemaker-us-west-1-851725529671\n",
      "mhist-vit/model.tar.gz LastModified: 2024-07-22 03:12:54+00:00 Size: 318708924\n",
      "pytorch-inference-2024-07-22-03-13-07-428/model.tar.gz LastModified: 2024-07-22 03:13:34+00:00 Size: 318973586\n",
      "pytorch-inference-2024-07-22-03-18-34-828/model.tar.gz LastModified: 2024-07-22 03:19:04+00:00 Size: 318973480\n"
     ]
    }
   ],
   "source": [
    "# List model artifacts in S3\n",
    "objects = s3.list_objects_v2(Bucket=bucket)\n",
    "print(f\"{objects['KeyCount']} model artifacts remaining in S3 bucket: {objects['Name']}\")\n",
    "for obj in objects.get('Contents', []): # returns [] by default\n",
    "    print(f\"{obj['Key']} LastModified: {obj['LastModified']} Size: {obj['Size']}\")\n",
    "\n",
    "# Output:\n",
    "\n",
    "# model artifact for PyTorchModel:\n",
    "# mhist-vit/model.tar.gz LastModified: 2024-07-22 03:12:54+00:00 Size: 318708924\n",
    "\n",
    "# model artifact for PyTorchModel Endpoint:\n",
    "# pytorch-inference-2024-07-22-03-13-07-428/model.tar.gz LastModified: 2024-07-22 03:13:34+00:00 Size: 318973586\n",
    "\n",
    "# s3.list_objects_v2 returns ResponseMetadata:\n",
    "# RequestId- same as x-amz-request-id (below)\n",
    "# HostId- host that responded (s3 id)\n",
    "# HTTPStatusCode- 200 for success\n",
    "# HTTPHeaders:\n",
    "#       x-amz-id-2: s3 id\n",
    "#       x-amz-request-id: AWS id for the request\n",
    "#       date\n",
    "#       x-amz-bucket-region\n",
    "#       content-type\n",
    "#       transfer-encoding: 'chunked' response\n",
    "#       server: 'AmazonS3'\n",
    "# RetryAttempts = 0\n",
    "# IsTruncated\n",
    "# Contents: (list of dicts)\n",
    "#       Key\n",
    "#       LastModified\n",
    "#       ETag\n",
    "#       Size\n",
    "# StorageClass = 'STANDARD'\n",
    "# Name = 'sagemaker-us-west-1-851725529671'\n",
    "# Prefix = ''\n",
    "# MaxKeys = 1000\n",
    "# EncodingType = url\n",
    "# KeyCount = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Endpoint Configurations\n",
    "sagemaker_client = sagemaker_session.sagemaker_client\n",
    "response = sagemaker_client.list_endpoint_configs()\n",
    "\n",
    "for endpoint_config in response['EndpointConfigs']:\n",
    "    print(f\"EndpointConfigName: {endpoint_config['EndpointConfigName']}, CreationTime: {endpoint_config['CreationTime']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433f4dbf-26a0-455b-a5e8-ac8901070baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optionally delete the SageMaker model, which doesn't incur charges\n",
    "# # (or go to SageMaker Studio --> Models --> Deployable Models)\n",
    "# model.delete_model()\n",
    "\n",
    "# # Delete model artifact from sagemaker.Session().default_bucket()\n",
    "# S3_PREFIX = 'mhist-vit'\n",
    "# S3_FILENAME = 'model.tar.gz'\n",
    "# s3.delete_object(Bucket=bucket, Key=f\"{S3_PREFIX}/{S3_FILENAME}\")\n",
    "\n",
    "# # Delete all artifacts from sagemaker.Session().default_bucket():\n",
    "# objects = s3.list_objects_v2(Bucket=bucket)\n",
    "# for obj in objects.get('Contents', []):\n",
    "#     s3.delete_object(Bucket=bucket, Key=obj['Key'])\n",
    "\n",
    "# # sagemaker.Session() object doesn't use any other resources (besides notebook memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbdad27",
   "metadata": {},
   "source": [
    "Also, remember to stop the Studio Instance:\n",
    "- Close this notebook, then click **SageMaker Studio --> Running Instances --> Stop**\n",
    "- When you stop the Studio instance, SageMaker with delete the associated EBS volume\n",
    "\n",
    "To double-check, go to the EC2 console\n",
    "- In the left sidebar, click Elastic Block Store --> Volumes\n",
    "- Look for any volumes with a name starting with \"sagemaker-\"\n",
    "\n",
    "Check **AWS Billing** dashboard to check for any resources that might be used accidentally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
